#### 卷积神经网络(CNN)

---

- 激活函数：在卷积操作之后添加激活函数，因为每一次卷积操作都是线性的(矩阵乘)，所以如果没有激活函数，所有卷积操作都可合并为一次卷积操作，这样只能提取线性特征。因此在卷积之后添加非线性的激活函数可以解决非线性建模的问题。
- 批归一化：目的是解决内部协变量偏移，可加在卷积核后、激活层前，也可加在激活层后。减少初始输入和参数变化的影响，对初值不那么敏感。
- 池化：暴力提取，不可逆。有最大池化，平均池化。
- 由于网络的前几层采用的是卷积层或是池化层，因此只能提取局部的信息，所以网络的后几层要使用全局连接的全连接层(近年来使用全局平均池化，可以降低计算量)，来获取局部与总体之间的关系。
- 损失函数$Loss(.,.)$：通过模型的预测值与真实值计算出损失值，然后用损失值去反向更新参数，使得预测值向真实值靠拢，从而达到学习的目的。



#### 循环神经网络(RNN)

---

- 隐藏状态：每轮循环都有各自的隐藏状态，输入$x_t$与上一时刻的隐藏状态$h_{t-1}$ 经过偏移和输入层激活函数计算出当前时刻的隐藏状态$h_t$，当前时刻的隐藏状态$h_t$再经过偏移和输出层激活函数计算出输出$y_t$。
- LSTM：长短期记忆网络，由于RNN存在梯度爆炸和梯度消失的问题，所以LSTM在隐藏状态层之前加入一个元胞状态单元来配合形成长短期记忆。
- Seq2Seq：采用编码器和解码器两个结构，中间用一个C连接。可能会出现信息丢失的问题，可采用倒序的编码顺序。



### 图神经网络(GNN)

---

- 拉普拉斯矩阵$L = D - A = U\Lambda U^T$,其中图的度矩阵$D = diag(d_1,d_2,...,d_n)$,图的邻接矩阵$A$





### 生成模型

---

##### 受限玻尔兹曼机

- 能量函数：描述系统的能量值，越小，表示系统越稳定。
- 采样：[0,1]之间的随机数向特定分布的映射。
- 

